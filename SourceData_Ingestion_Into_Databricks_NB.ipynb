{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This notebook shows you how to create and query a table or DataFrame loaded from data stored in Azure Blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84d079e4-93e8-460d-9d95-54a3fa326ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 1: Set the data location and type\n",
    "\n",
    "There are two ways to access Azure Blob storage: account keys and shared access signatures (SAS).\n",
    "\n",
    "To get started, we need to set the location and type of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c760fc54-dbf0-4715-8df6-8fdff699c029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"haridatacontainer\"\n",
    "storage_account_access_key = \"3***w==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"wasbs://sourcedata@haridatacontainer.blob.core.windows.net/products.csv\"\n",
    "file_type = \"csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acc0dda-69e3-4817-bf5d-450562afe113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n",
    "  storage_account_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51114086-25f5-4c9c-8bb3-64ff28b0f0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 2: Read the data\n",
    "\n",
    "Now that we have specified our file metadata, we can create a DataFrame. Notice that we use an *option* to specify that we want to infer the schema from the file. We can also explicitly set this to a particular schema if we have one already.\n",
    "\n",
    "First, let's create a DataFrame in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011d88c8-e444-4834-9f91-2ea27aea6c68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Option 1 : Using \"COPY INTO\"\n",
    "Pre-requisite : Generate shared access signature (SAS) tokensi n the Azure portal Â· Right-click the container or file and select Generate SAS from the drop-down menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d4fdca-fd89-4b24-b744-c87434da85df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DROP TABLE products_agg2;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS products_agg2;\n",
    "\n",
    "COPY INTO products_agg2\n",
    "FROM 'wasbs://sourcedata@haridatacontainer.blob.core.windows.net/products.csv'\n",
    "WITH (\n",
    "  CREDENTIAL (AZURE_SAS_TOKEN = 'sp=XXX3D')\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true', 'mergeSchema' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    ";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d4f80c-c83b-46a3-8fde-378338a63aea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option2 : Using \"spark.read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6366891-7da1-478e-8094-4291f4fca976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(file_type).option(\"inferSchema\", \"true\").option(\"header\",\"true\").load(file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92108b48-e8f5-4d0d-9d24-3775b36ca207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 3: Query the data\n",
    "\n",
    "Now that we have created our DataFrame, we can query it. For instance, you can identify particular columns to select and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e798450b-e281-42a7-9a78-51bb4bd07540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec0eb47-1220-48a6-81b4-adb261a7265a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Category\",\"BikeCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901532bd-4e23-4ec8-86d7-611037586ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8abe7239-0632-4b90-baf0-38549b038a38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Step 4: (Optional) Create a view or table\n",
    "\n",
    "If you want to query this data as a table, you can simply register it as a *view* or a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f300bc-d77c-4cea-b950-fdebb696a3a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"src_products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8abbb176-6708-4684-86df-50e35b77d223",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can query this view using Spark SQL. For instance, we can perform a simple aggregation. Notice how we can use `%sql` to query the view from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41b3f47-6033-4512-816a-1bff6b94fae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT ProductName, AVG(ListPrice) FROM src_products GROUP BY ProductName ORDER BY AVG(ListPrice) DESC LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4f71a4-2c5b-4df4-8b5b-16b9c17c057a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Since this table is registered as a temp view, it will be available only to this notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73dea27f-997b-4999-90ab-076048bb82e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sqldf.write.format(\"parquet\").saveAsTable(\"products_agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0880bffd-4f8d-4565-8428-ace92f92a53c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This table will persist across cluster restarts and allow various users across different notebooks to query this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc916ca-14c1-4084-99d4-defcfb9f54b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SHOW CREATE TABLE default.products_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3854b0b-a20b-4f36-a2e0-f9e39d691620",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write the transformed data back to Azure Data Lake Storage Gen2 for further processing(say via Azure Data Factory or Azure Synapse Analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"wasbs://sourcedata@haridatacontainer.blob.core.windows.net/transformed_products_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf6b422-758d-4ec2-a556-4ee232708d6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").save(\"wasbs://sourcedata@haridatacontainer.blob.core.windows.net/transformed_products_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e930a21-9337-49ee-9e1b-ce48afddb055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 147513422488020,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SourceData_Ingestion_Into_Databricks_NB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
